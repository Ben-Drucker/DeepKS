<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Getting Started</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

/* From extension ms-toolsai.jupyter */
/* These classnames are inherited from bootstrap, but are present in most notebook renderers */

.alert {
    width: auto;
    padding: 1em;
    margin-top: 1em;
    margin-bottom: 1em;
}
.alert > *:last-child {
    margin-bottom: 0;
}
#preview > .alert:last-child {
    /* Prevent this being set to zero by the default notebook stylesheet */
    padding-bottom: 1em;
}

.alert-success {
    /* Note there is no suitable color available, so we just copy "info" */
    background-color: var(--theme-info-background);
    color: var(--theme-info-foreground);
}
.alert-info {
    background-color: var(--theme-info-background);
    color: var(--theme-info-foreground);
}
.alert-warning {
    background-color: var(--theme-warning-background);
    color: var(--theme-warning-foreground);
}
.alert-danger {
    background-color: var(--theme-error-background);
    color: var(--theme-error-foreground);
}

</style>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        
    </head>
    <body class="vscode-body vscode-light">
        <style>
    pre.bash-output.bash-output{
        background-color: #ebe9c27f;
    }
    code.inline-bash-output{
        background-color: #ebe9c27f;
    }
    code{
        background-color: rgba(220, 220, 220, 0.4);
        padding: 1px 3px;
        border-radius: 5px;
    }
    pre code{
        background-color: transparent;
        padding: 0;
        border-radius: 0;
    }
    h1{
        border-bottom-width: 2px;
    }
    
    h2{
        border-bottom-width: 1px;
        border-bottom-color: #00000040;
        border-bottom-style: solid;
    }

    h3{
        border-bottom-width: 1px;
        border-bottom-color: #00000040;
        border-bottom-style: dashed;
    }
</style>
<h1 id="getting-started">Getting Started</h1>
<p>The bulk of the DeepKS tool is run through Docker. It will essentially run like it would in a virtual machine. This makes dependency management a breeze. Follow the steps below to get started. One neednot clone the DeepKS Git repository to use the tool.</p>
<h2 id="general-notes-relating-to-devices-read-before-running-any-program">General Notes Relating to Devices (Read before running any program)</h2>
<h3 id="does-my-computer-have-a-cuda-compatible-gpu">Does My Computer Have a CUDA-compatible GPU?</h3>
<p>If you're not sure, follow the instructions <a href="https://askubuntu.com/a/1273434">here</a>.</p>
<h3 id="running-on-personal-computer-with-cuda">Running On Personal Computer with CUDA</h3>
<p>If you have a CUDA-compatible GPU, you can run the program on your personal computer and take advantage of the GPU. This is the fastest way to run the program (besides using an HPC cluster). There is some additional setup involved. If you want to bypass this setup, you can run the program without CUDA on your personal computer or on a HPC cluster (see below). But if you do want to run the program with CUDA on your personal computer, do the following:</p>
<ol>
<li>Go through the steps in the auxillary help page <a href="https://gitlab.com/Ben-Drucker/deepks/-/blob/main/build/cuda_installation.html">cuda_installation.md</a>.</li>
</ol>
<h3 id="running-on-personal-computer-without-cuda">Running On Personal Computer without CUDA</h3>
<ol>
<li>Download Docker here <a href="https://www.docker.com/products/docker-desktop/">https://www.docker.com/products/docker-desktop/</a> and follow the installation instructions for your operating system.</li>
</ol>
<h3 id="running-on-hpc-cluster">Running On HPC Cluster</h3>
<p><em><strong>Note: These instructions are specific to the PNNL &quot;deception&quot; cluster (it assumes <code>module</code> and <code>apptainer</code> are preinstalled). It also assumes you have an active account.</strong></em></p>
<ol>
<li>Open a terminal SSH into the cluster with <code>ssh &lt;username&gt;@deception.pnnl.gov</code>, making sure to replace <code>&lt;username&gt;</code> with your actual username.</li>
<li>Run <code>module load apptainer</code> to load Apptainer.</li>
<li>Run <code>apptainer pull benndrucker/deepks:latest</code> to pull the Docker image.</li>
<li>Run <code>apptainer shell --nv benndrucker/deepks:latest</code> to start the Docker container (in Apptainer).</li>
</ol>
<h2 id="terminology"> Terminology </h2>
Please read this explanation: "[An image is a blueprint for a snapshot of a 'system-in-a-system' (similar to a virtual machine).] An instance of an image is called a container...If you start this image, you have a running container of this image. You can have many running containers of the same image." ~ <a href="https://stackoverflow.com/a/23736802/16158339">Thomas Uhrig and Alex Telon's post</a>
<h2 id="pull-docker-image">Pull Docker Image</h2>
<!--TODO: Credentials-->
<ol>
<li>
<ul>
<li>If running on a personal computer, ensure Docker Desktop (Installed above) is running and a terminal is open.</li>
<li>If using WSL on Windows, ensure WSL is running.</li>
<li>If using HPC cluster, ensure you are SSH'd into the cluster and have run <code>module load apptainer</code>.</li>
</ul>
</li>
<li>Run the following command to start the docker session: <code>docker run -it benndrucker/deepks:latest</code></li>
<li>A command prompt should appear and look like <code class = "inline-bash-output">root@shahash:/#</code>, where <code>shahash</code> is a hexadecimal of the Docker Container. You are now inside the Docker Container at the top-level <code>/</code> directory. See the steps below to run various programs <em>from this prompt</em>.</li>
</ol>
<h2 id="reuse-docker-container">Reuse Docker Container</h2>
<ol>
<li>To resuse the created container (so that any saved state is available), run <code>docker ps -a</code>. This will show a list of all running and previously-created containers.</li>
<li>Copy the hash id of the desired container.</li>
<li>Run <code>docker run -td &lt;copied hash&gt;</code> (making sure to replace <code>&lt;copied hash&gt;</code> with the hexadecimal hash you actually copied). Once this is complete, run <code>docker exec -it &lt;copied hash&gt;</code> (again, copying in the actual hash). This will give you the command prompt inside the Docker container.</li>
</ol>
<h1 id="running-the-programs">Running The Programs</h1>
<p><em><strong>Note: The following steps are run from <u> inside the Docker container</u>. See the steps above to start the Docker container.</strong></em></p>
<h2 id="using-api">Using API</h2>
<h2 id="from-command-line">From Command Line</h2>
<p>The Command Line Interface is the main way to query the deep learning model. The API is a submodule of <code>DeepKS</code>. (Henceforth referred to as <code>DeepKS.api</code>.) The <code>DeepKS.api</code> module, itself contains a submodule <code>main</code>. (Henceforth referred to as <code>DeepKS.api.main</code>). This is the main &quot;entrypoint&quot; for running queries. Because of various Python specifications, <code>DeepKS.api.main</code> must be run as a module from <em>outside</em> the <code>/DeepKS</code> directory. Hence, to run from the command line in the Docker container, run</p>
<pre><code class="language-bash"><span class="hljs-built_in">cd</span> /
python -m DeepKS.api.main [options]
</code></pre>
<p>where <code>[options]</code> are the options you wish to pass to the program. To see the required and optional arguments, run <code>python -m DeepKS.api.main --help</code>.</p>
<p>When you run this, it will print</p>
<pre><code class="language-bash">usage: python -m DeepKS.api.main [-h] (-k &lt;kinase sequences&gt; | -kf &lt;kinase sequences file&gt;)
                                 (-s &lt;site sequences&gt; | -sf &lt;site sequences file&gt;)
                                 [-p {in_order,dictionary,in_order_json,dictionary_json}] [-v]
                                 [--pre_trained_nn &lt;pre-trained neural network file&gt;]
                                 [--pre_trained_gc &lt;pre-trained group classifier file&gt;]
</code></pre>
<ul>
<li>Anything in square brackets is optional.</li>
<li>For each instance of round parentheses, you must provide one of the options between &quot;<code>|</code>&quot;.</li>
<li>Curly braces show available options for a flag.</li>
</ul>
<p>With that in mind, here are some examples of how to run the program (make sure to be in the top-level <code>/</code> directory):</p>
<pre><code class="language-bash">python -m DeepKS.api.main -kf my/kinase/sequences.txt -sf my/site/sequences.txt -p in_order_json -v True

python -m DeepKS.api.main -k KINASESEQ1,KINASESEQ2,KINASESEQ3 -s SITESEQ1,SITESEQ2,SITESEQ3 -p dictionary

python -m DeepKS.api.main -kf my/kinase/sequences.txt -s SITESEQ1,SITESEQ2,SITESEQ3 -p in_order -v False

python -m DeepKS.api.main -kf my/kinase/sequences.txt -sf my/site/sequences.txt
</code></pre>
<h2 id="as-a-python-import">As a Python Import</h2>
<p>It is recommended to clone any external Git repositories to a directory inside the Docker container...TODO -- incomplete</p>
<h2 id="api-specification">API Specification</h2>
<h3 id="functions-of-deepksapimain">Functions of DeepKS.api.main:</h3>
<pre><code class="language-python">make_predictions(kinase_seqs, site_seqs, predictions_output_format, verbose, pre_trained_gc, pre_trained_nn):
    <span class="hljs-string">&quot;&quot;&quot;Make a target/decoy prediction for a kinase-substrate pair.

    Args:
        kinase_seqs (list[str]): The kinase sequences. Each must be &lt;= 4128 residues long.
        site_seqs ([str]): The site sequences. Each must be 15 residues long.
        predictions_output_format (str, optional): The format of the output. Defaults to &quot;in_order&quot;.
            - &quot;in_order&quot; returns a list of predictions in the same order as the input kinases and sites.
            - &quot;dictionary&quot; returns a dictionary of predictions, where the keys are the input kinases and sites and the values are the predictions.
            - &quot;in_order_json&quot; outputs a JSON string (filename = ../out/current-date-and-time.json of a list of predictions in the same order as the input kinases and sites.
            - &quot;dictionary_json&quot; outputs a JSON string (filename = ../out/current-date-and-time.json) of a dictionary of predictions, where the keys are the input kinases and sites and the values are the predictions.
        verbose (bool, optional): Whether to print predictions. Defaults to True.
        pre_trained_gc (str, optional): Path to previously trained group classifier model state. Defaults to &quot;data/bin/deepks_weights.0.0.1.pt&quot;.
        pre_trained_nn (str, optional): Path to previously trained neural network model state. Defaults to &quot;data/bin/deepks_weights.0.0.1.pt&quot;.
    
    Returns:
        None, or dictionary, or list, depending on `predictions_output_format`
    &quot;&quot;&quot;</span>

parse_api():
    <span class="hljs-string">&quot;&quot;&quot;Parse the command line arguments.

    Returns:
        dict[str, Any]: Dictionary mapping the argument name to the argument value.
    &quot;&quot;&quot;</span>
</code></pre>
<h1 id="reproducing-everything-from-scratch">Reproducing Everything From Scratch</h1>
<p>TODO -- still working on cleaning things up.</p>
<h2 id="preprocessing-and-data-collection">Preprocessing and Data Collection</h2>
<h2 id="training">Training</h2>
<p>The python training scripts contain command line interfaces. However, to make running easier, one can use the bash scripts in the <code>models</code> directory. The bash scripts are simply wrappers around the python scripts. The bash scripts are the recommended way to run the training scripts.</p>
<ol>
<li>Run <code>bash models/train_multi_stage_classifier.sh</code> to train the multi-stage classifier.</li>
</ol>
<h2 id="evaluating">Evaluating</h2>
<h2 id="creating-evaluation-diagrams">Creating Evaluation Diagrams</h2>
<h2 id="creating-other-diagrams">Creating Other Diagrams</h2>

        
        
    </body>
    </html>